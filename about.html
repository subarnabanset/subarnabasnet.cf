<link rel="stylesheet" type="text/css" href="ls.css">
<h1>History</h1>
See also: History of programming languages
Programmable devices have existed at least as far back as 1206 AD, when the automata of Al-Jazari were programmable, via pegs and cams, to play various rhythms and drum patterns;[3] and the 1801 Jacquard loom could produce entirely different weaves by changing the "program" - a series of pasteboard cards with holes punched in them.

However, the first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.[4] Women would continue to dominate the field of computer programming until the mid 1960s.[5]


Data and instructions were once stored on external punched cards, which were kept in order and arranged in program decks.
In the 1880s Herman Hollerith invented the concept of storing data in machine-readable form.[6] Later a control panel (plugboard) added to his 1906 Type I Tabulator allowed it to be programmed for different jobs, and by the late 1940s, unit record equipment such as the IBM 602 and IBM 604, were programmed by control panels in a similar way; as were the first electronic computers. However, with the concept of the stored-program computers introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.[citation needed]

Machine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation. Assembly languages were soon developed that let the programmer specify instruction in a text format, (e.g., ADD X, TOTAL), with abbreviations for each operation code and meaningful names for specifying addresses. However, because an assembly language is little more than a different notation for a machine language, any two machines with different instruction sets also have different assembly languages. Kathleen Booth created one of the first Assembly languages in 1950 for various computers at Birkbeck College.[7]


Wired control panel for an IBM 402 Accounting Machine.
High-level languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it targetable to varying machine instruction sets via compilation declarations and heuristics. The first compiler for a programming language was developed by Grace Hopper.[8] When Hopper went to work on UNIVAC in 1949, she brought the idea of using compilers with her.[9][10] Compilers harness the power of computers to make programming easier[11] by allowing programmers to specify calculations by entering a formula using infix notation (e.g., Y = X*2 + 5*X + 9) for example. FORTRAN, the first widely used high-level language to have a functional implementation which permitted the abstraction of reusable blocks of code, came out in 1957.[12] In 1951 Frances E. Holberton developed the first sort-merge generator which ran on the UNIVAC I.[13] Another woman working at UNIVAC, Adele Mildred Koss, developed a program that was a precursor to report generators.[13] In Russia, Kateryna Yushchenko developed the Address programming language for the MESM in 1955.[14]

The idea for the creation of COBOL started in 1959 when Mary K. Hawes, who worked for Burroughs Corporation, set up a meeting to discuss creating a common business language.[15] She invited six people, including Grace Hopper.[15] Hopper was involved in developing COBOL as a business language and creating "self-documenting" programming.[16][17] Hopper's contribution to COBOL was based on her programming language, called FLOW-MATIC.[10] In 1961, Jean E. Sammet developed FORMAC and also published Programming Languages: History and Fundamentals which went on to be a standard work on programming languages.[15][18]

Programs were mostly still entered using punched cards or paper tape. See computer programming in the punch card era. By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers. Frances Holberton created a code to allow keyboard inputs while she worked at UNIVAC.[19] Text editors were developed that allowed changes and corrections to be made much more easily than with punched cards. Sister Mary Kenneth Keller worked on developing the programming language, BASIC which she was a graduate student at Dartmouth in the 1960s.[20] One of the first object-oriented programming languages, Smalltalk, was developed by seven programmers, including Adele Goldberg, in the 1970s.[21] In 1985, Radia Perlman developed the Spinning Tree Protocol in order to route packets of network information efficiently.[22][23]

Modern programming

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Computer programming" – news · newspapers · books · scholar · JSTOR (August 2010) (Learn how and when to remove this template message)
Quality requirements
Software development
Core activities
Processes Requirements Design Engineering Construction Testing Debugging Deployment Maintenance
Paradigms and models
Agile Cleanroom Incremental Prototyping Spiral V model Waterfall
Methodologies and frameworks
ASD DevOps DAD DSDM FDD IID Kanban Lean SD LeSS MDD MSF PSP RAD RUP SAFe Scrum SEMAT TSP UP XP
Supporting disciplines
Configuration management Documentation Software quality assurance (SQA) Project management User experience
Practices
ATDD BDD CCO CI CD DDD PP SBE Stand-up TDD
Tools
Compiler Debugger Profiler GUI designer Modeling IDE Build automation Release automation Infrastructure as code Testing
Standards and Bodies of Knowledge
BABOK CMMI IEEE standards ISO 9001 ISO/IEC standards PMBOK SWEBOK ITIL
Glossaries
Artificial intelligence Computer science Electrical and electronics engineering
vte
Whatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most important:

Reliability: how often the results of a program are correct. This depends on conceptual correctness of algorithms, and minimization of programming mistakes, such as mistakes in resource management (e.g., buffer overflows and race conditions) and logic errors (such as division by zero or off-by-one errors).
Robustness: how well a program anticipates problems due to errors (not bugs). This includes situations such as incorrect, inappropriate or corrupt data, unavailability of needed resources such as memory, operating system services and network connections, user error, and unexpected power outages.
Usability: the ergonomics of a program: the ease with which a person can use the program for its intended purpose or in some cases even unanticipated purposes. Such issues can make or break its success even regardless of other issues. This involves a wide range of textual, graphical and sometimes hardware elements that improve the clarity, intuitiveness, cohesiveness and completeness of a program's user interface.
Portability: the range of computer hardware and operating system platforms on which the source code of a program can be compiled/interpreted and run. This depends on differences in the programming facilities provided by the different platforms, including hardware and operating system resources, expected behavior of the hardware and operating system, and availability of platform specific compilers (and sometimes libraries) for the language of the source code.
Maintainability: the ease with which a program can be modified by its present or future developers in order to make improvements or customizations, fix bugs and security holes, or adapt it to new environments. Good practices[24] during initial development make the difference in this regard. This quality may not be directly apparent to the end user but it can significantly affect the fate of a program over the long term.
Efficiency/performance: Measure of system resources a program consumes (processor time, memory space, slow devices such as disks, network bandwidth and to some extent even user interaction): the less, the better. This also includes careful management of resources, for example cleaning up temporary files and eliminating memory leaks.
<h1>computer</h1>
Main article: History of computer science
History of computing
Hardware
Hardware before 1960 Hardware 1960s to present
Software
Software Unix Free software and open-source software
Computer science
Artificial intelligence Compiler construction Computer science Operating systems Programming languages Prominent pioneers Software engineering
Modern concepts
General-purpose CPUs Graphical user interface Internet Laptops Personal computers Video games World Wide Web
By country
Bulgaria Poland Romania Soviet Bloc Soviet Union Yugoslavia
Timeline of computing
before 1950 1950–1979 1980–1989 1990–1999 2000–2009 2010–2019 more timelines ...
Glossary of computer science
Category Category
vte

Charles Babbage, sometimes referred to as the "father of computing".[2]

Ada Lovelace is often credited with publishing the first algorithm intended for processing on a computer.[3]
The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.

Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623.[4] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[5] He may be considered the first computer scientist and information theorist, for, among other reasons, documenting the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he released his simplified arithmometer, which was the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[6] He started developing this machine in 1834, and "in less than two years, he had sketched out many of the salient features of the modern computer".[7] "A crucial step was the adoption of a punched card system derived from the Jacquard loom"[7] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first computer program.[8] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[9] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".[10]

During the 1940s, as new and more powerful computing machines were developed, the term computer came to refer to the machines rather than their human predecessors.[11] As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world. [12] Ultimately, the close relationship between IBM and the university was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. [13]Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s.[14][15] The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science degree program in the United States was formed at Purdue University in 1962.[16] Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.

Although many initially believed it was impossible that computers themselves could actually be a scientific field of study, in the late fifties it gradually became accepted among the greater academic population.[17][18] It is the now well-known IBM brand that formed part of the computer science revolution during this time. IBM (short for International Business Machines) released the IBM 704[19] and later the IBM 709[20] computers, which were widely used during the exploration period of such devices. "Still, working with the IBM [computer] was frustrating […] if you had misplaced as much as one letter in one instruction, the program would crash, and you would have to start the whole process over again".[17] During the late 1950s, the computer science discipline was very much in its developmental stages, and such issues were commonplace.[18]

Time has seen significant improvements in the usability and effectiveness of computing technology.[21] Modern society has seen a significant shift in the users of computer technology, from usage only by experts and professionals, to a near-ubiquitous user base. Initially, computers were quite costly, and some degree of humanitarian aid was needed for efficient use—in part from professional computer operators. As computer adoption became more widespread and affordable, less human assistance was needed for common usage.

See also: History of computing and History of informatics
Contributions

The German military used the Enigma machine (shown here) during World War II for communications they wanted kept secret. The large-scale decryption of Enigma traffic at Bletchley Park was an important factor that contributed to Allied victory in WWII.[22]
Despite its short history as a formal academic discipline, computer science has made a number of fundamental contributions to science and society—in fact, along with electronics, it is a founding science of the current epoch of human history called the Information Age and a driver of the information revolution, seen as the third major leap in human technological progress after the Industrial Revolution (1750–1850 CE) and the Agricultural Revolution (8000–5000 BC).

These contributions include:

The start of the "Digital Revolution", which includes the current Information Age and the Internet.[23]
A formal definition of computation and computability, and proof that there are computationally unsolvable and intractable problems.[24]
The concept of a programming language, a tool for the precise expression of methodological information at various levels of abstraction.[25]
In cryptography, breaking the Enigma code was an important factor contributing to the Allied victory in World War II.[22]
Scientific computing enabled practical evaluation of processes and situations of great complexity, as well as experimentation entirely by software. It also enabled advanced study of the mind, and mapping of the human genome became possible with the Human Genome Project.[23] Distributed computing projects such as Folding@home explore protein folding.
Algorithmic trading has increased the efficiency and liquidity of financial markets by using artificial intelligence, machine learning, and other statistical and numerical techniques on a large scale.[26] High frequency algorithmic trading can also exacerbate volatility.[27]
Computer graphics and computer-generated imagery have become ubiquitous in modern entertainment, particularly in television, cinema, advertising, animation and video games. Even films that feature no explicit CGI are usually "filmed" now on digital cameras, or edited or post-processed using a digital video editor.[28][29]
Simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.[citation needed]
Artificial intelligence is becoming increasingly important as it gets more efficient and complex. There are many applications of AI, some of which can be seen at home, such as robotic vacuum cleaners. It is also present in video games and on the modern battlefield in drones, anti-missile systems, and squad support robots.
Human–computer interaction combines novel algorithms with design strategies that enable rapid human performance, low error rates, ease in learning, and high satisfaction. Researchers use ethnographic observation and automated data collection to understand user needs, then conduct usability tests to refine designs. Key innovations include the direct manipulation, selectable web links, touchscreen designs, mobile applications, and virtual reality.
<h1>website</h1>
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
This article is about websites in general. For the Internet domain .website, see List of Internet top-level domains.
Not to be confused with WebCite.

The usap.gov website
A website[1] is a collection of related web pages, including multimedia content, typically identified with a common domain name, and published on at least one web server. Notable examples are wikipedia.org, google.com, and amazon.com.

A website may be accessible via a public Internet Protocol (IP) network, such as the Internet, or a private local area network (LAN), by referencing a uniform resource locator (URL) that identifies the site.

Websites can have many functions and can be used in various fashions; a website can be a personal website, a corporate website for a company, a government website, an organization website, etc. Websites are typically dedicated to a particular topic or purpose, ranging from entertainment and social networking to providing news and education. All publicly accessible websites collectively constitute the World Wide Web, while private websites, such as a company's website for its employees, are typically a part of an intranet.

Web pages, which are the building blocks of websites, are documents, typically composed in plain text interspersed with formatting instructions of Hypertext Markup Language (HTML, XHTML). They may incorporate elements from other websites with suitable markup anchors. Web pages are accessed and transported with the Hypertext Transfer Protocol (HTTP), which may optionally employ encryption (HTTP Secure, HTTPS) to provide security and privacy for the user. The user's application, often a web browser, renders the page content according to its HTML markup instructions onto a display terminal.

Hyperlinking between web pages conveys to the reader the site structure and guides the navigation of the site, which often starts with a home page containing a directory of the site web content. Some websites require user registration or subscription to access content. Examples of subscription websites include many business sites, news websites, academic journal websites, gaming websites, file-sharing websites, message boards, web-based email, social networking websites, websites providing real-time stock market data, as well as sites providing various other services. End users can access websites on a range of devices, including desktop and laptop computers, tablet computers, smartphones and smart TVs.